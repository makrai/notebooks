{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "#! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity of fixed-length models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity (PPL) is one of the most common metrics for evaluating language models. Before diving in, we should note\n",
    "that the metric applies specifically to classical language models (sometimes called autoregressive or causal language\n",
    "models) and is not well defined for masked language models like BERT (see [summary of the models](https://huggingface.co/docs/transformers/main/en/model_summary)).\n",
    "\n",
    "Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. If we have a tokenized\n",
    "sequence $X = (x_0, x_1, \\dots, x_t)$, then the perplexity of $X$ is,\n",
    "\n",
    "$$\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\}$$\n",
    "\n",
    "where $\\log p_\\theta (x_i|x_{<i})$ is the log-likelihood of the ith token conditioned on the preceding tokens $x_{<i}$ according to our model. Intuitively, it can be thought of as an evaluation of the model's ability to predict uniformly among the set of specified tokens in a corpus. Importantly, this means that the tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.\n",
    "\n",
    "This is also equivalent to the exponentiation of the cross-entropy between the data and model predictions. For more\n",
    "intuition about perplexity and its relationship to Bits Per Character (BPC) and data compression, check out this\n",
    "[fantastic blog post on The Gradient](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating PPL with fixed-length models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we weren't limited by a model's context size, we would evaluate the model's perplexity by autoregressively\n",
    "factorizing a sequence and conditioning on the entire preceding subsequence at each step, as shown below.\n",
    "\n",
    "<img width=\"600\" alt=\"Full decomposition of a sequence with unlimited context length\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif\"/>\n",
    "\n",
    "When working with approximate models, however, we typically have a constraint on the number of tokens the model can\n",
    "process. The largest version of [GPT-2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2), for example, has a fixed length of 1024 tokens, so we\n",
    "cannot calculate $p_\\theta(x_t|x_{<t})$ directly when $t$ is greater than 1024.\n",
    "\n",
    "Instead, the sequence is typically broken into subsequences equal to the model's maximum input size. If a model's max\n",
    "input size is $k$, we then approximate the likelihood of a token $x_t$ by conditioning only on the\n",
    "$k-1$ tokens that precede it rather than the entire context. When evaluating the model's perplexity of a\n",
    "sequence, a tempting but suboptimal approach is to break the sequence into disjoint chunks and add up the decomposed\n",
    "log-likelihoods of each segment independently.\n",
    "\n",
    "<img width=\"600\" alt=\"Suboptimal PPL not taking advantage of full available context\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif\"/>\n",
    "\n",
    "This is quick to compute since the perplexity of each segment can be computed in one forward pass, but serves as a poor\n",
    "approximation of the fully-factorized perplexity and will typically yield a higher (worse) PPL because the model will\n",
    "have less context at most of the prediction steps.\n",
    "\n",
    "Instead, the PPL of fixed-length models should be evaluated with a sliding-window strategy. This involves repeatedly\n",
    "sliding the context window so that the model has more context when making each prediction.\n",
    "\n",
    "<img width=\"600\" alt=\"Sliding window PPL taking advantage of all available context\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif\"/>\n",
    "\n",
    "This is a closer approximation to the true decomposition of the sequence probability and will typically yield a more\n",
    "favorable score. The downside is that it requires a separate forward pass for each token in the corpus. A good\n",
    "practical compromise is to employ a strided sliding window, moving the context by larger strides rather than sliding by\n",
    "1 token a time. This allows computation to proceed much faster while still giving the model a large context to make\n",
    "predictions at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Calculating perplexity with GPT-2 in ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate this process with GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pysrt\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ab2b0f22eb4c72ac75e6fa3ad0fdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt_neox to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbff25453204b7caa7f156fe4cc8326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/42.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d95eaa347141a5a193be4e88101450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d711ea8ca0294bb989f1b1a7f04aa230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a492e433d86419584f292749049ff74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebab355398a4d8bbf89805c26292f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at NYTK/PULI-GPT-3SX and are newly initialized: ['h.1.ln_1.weight', 'h.25.ln_2.bias', 'h.12.attn.c_attn.bias', 'h.15.attn.c_attn.bias', 'h.22.mlp.c_proj.weight', 'h.29.attn.c_attn.weight', 'h.22.ln_1.weight', 'h.20.mlp.c_fc.bias', 'h.24.attn.c_attn.weight', 'h.17.attn.c_proj.bias', 'h.6.mlp.c_proj.weight', 'h.10.mlp.c_fc.weight', 'h.2.attn.c_attn.weight', 'h.28.attn.c_attn.bias', 'h.10.ln_1.weight', 'h.7.ln_1.weight', 'h.14.attn.c_proj.weight', 'h.30.mlp.c_fc.bias', 'h.13.mlp.c_fc.bias', 'h.4.mlp.c_proj.weight', 'h.24.mlp.c_fc.weight', 'h.8.ln_2.weight', 'h.21.attn.c_proj.bias', 'h.20.mlp.c_proj.bias', 'h.14.attn.c_attn.weight', 'h.25.ln_2.weight', 'h.9.attn.c_attn.weight', 'h.3.attn.c_attn.weight', 'h.28.attn.c_attn.weight', 'h.13.ln_1.bias', 'h.27.attn.c_attn.bias', 'h.9.ln_1.weight', 'h.13.mlp.c_proj.weight', 'h.21.mlp.c_proj.weight', 'h.19.mlp.c_proj.weight', 'h.17.attn.c_proj.weight', 'h.16.attn.c_proj.bias', 'h.14.ln_2.weight', 'h.31.ln_2.weight', 'h.2.ln_1.weight', 'h.31.mlp.c_fc.weight', 'h.26.mlp.c_fc.bias', 'h.29.attn.c_attn.bias', 'h.3.mlp.c_fc.bias', 'h.30.attn.c_proj.weight', 'h.17.ln_2.bias', 'h.9.ln_1.bias', 'h.2.ln_2.weight', 'h.0.mlp.c_proj.bias', 'h.1.ln_1.bias', 'h.14.mlp.c_proj.bias', 'h.9.attn.c_attn.bias', 'h.15.attn.c_proj.weight', 'h.3.ln_2.bias', 'h.23.ln_2.weight', 'h.30.ln_2.weight', 'h.25.mlp.c_fc.bias', 'h.4.attn.c_attn.bias', 'h.6.attn.c_attn.bias', 'h.16.attn.c_attn.bias', 'h.16.ln_2.bias', 'h.25.attn.c_attn.bias', 'h.29.mlp.c_proj.bias', 'h.26.attn.c_proj.weight', 'h.1.mlp.c_fc.weight', 'h.4.mlp.c_proj.bias', 'h.17.ln_2.weight', 'h.23.mlp.c_proj.bias', 'h.11.attn.c_attn.bias', 'h.14.ln_2.bias', 'h.16.mlp.c_proj.weight', 'h.8.mlp.c_proj.bias', 'h.12.mlp.c_fc.bias', 'h.24.ln_1.bias', 'h.9.attn.c_proj.bias', 'h.17.ln_1.weight', 'h.5.mlp.c_proj.bias', 'h.9.mlp.c_proj.weight', 'h.18.ln_1.weight', 'h.29.ln_1.bias', 'h.15.mlp.c_fc.bias', 'h.12.attn.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.17.attn.c_attn.weight', 'h.31.ln_1.bias', 'h.2.attn.c_attn.bias', 'h.28.ln_1.weight', 'h.30.attn.c_proj.bias', 'h.1.attn.c_proj.bias', 'h.1.ln_2.bias', 'h.7.attn.c_proj.bias', 'h.25.attn.c_attn.weight', 'h.23.mlp.c_fc.weight', 'h.9.mlp.c_proj.bias', 'h.5.attn.c_proj.bias', 'h.2.attn.c_proj.bias', 'h.12.ln_2.weight', 'h.25.attn.c_proj.bias', 'h.19.mlp.c_fc.weight', 'h.6.attn.c_attn.weight', 'h.19.attn.c_proj.bias', 'h.20.ln_2.weight', 'h.4.attn.c_attn.weight', 'h.11.mlp.c_proj.weight', 'h.26.mlp.c_proj.bias', 'h.3.attn.c_proj.bias', 'h.12.ln_1.bias', 'h.7.mlp.c_fc.bias', 'h.17.attn.c_attn.bias', 'h.22.ln_2.weight', 'h.28.ln_2.weight', 'h.6.attn.c_proj.bias', 'h.4.attn.c_proj.weight', 'h.27.mlp.c_fc.weight', 'h.23.attn.c_attn.weight', 'h.14.ln_1.weight', 'h.23.ln_1.bias', 'h.5.attn.c_proj.weight', 'h.10.mlp.c_proj.weight', 'h.11.attn.c_attn.weight', 'h.22.attn.c_attn.bias', 'h.10.attn.c_proj.bias', 'h.28.mlp.c_proj.weight', 'h.16.ln_2.weight', 'h.29.mlp.c_fc.weight', 'h.22.ln_1.bias', 'h.15.ln_1.weight', 'h.10.ln_2.bias', 'h.10.attn.c_attn.bias', 'h.1.ln_2.weight', 'h.29.ln_1.weight', 'h.18.ln_1.bias', 'h.12.ln_1.weight', 'h.12.attn.c_attn.weight', 'h.5.ln_1.bias', 'h.20.attn.c_proj.bias', 'h.18.mlp.c_fc.weight', 'h.3.ln_1.weight', 'h.19.attn.c_attn.weight', 'h.18.attn.c_proj.weight', 'h.11.mlp.c_fc.bias', 'h.24.attn.c_proj.weight', 'h.16.mlp.c_fc.weight', 'h.22.attn.c_proj.weight', 'h.6.ln_2.bias', 'h.21.mlp.c_fc.bias', 'h.24.ln_2.bias', 'h.24.mlp.c_fc.bias', 'h.30.ln_1.bias', 'h.3.attn.c_proj.weight', 'h.25.mlp.c_fc.weight', 'h.8.attn.c_attn.weight', 'h.8.attn.c_proj.bias', 'h.1.mlp.c_fc.bias', 'h.17.ln_1.bias', 'h.13.attn.c_attn.bias', 'h.2.mlp.c_fc.weight', 'h.7.mlp.c_fc.weight', 'h.2.ln_2.bias', 'h.28.ln_2.bias', 'h.3.ln_1.bias', 'h.21.ln_2.bias', 'h.21.ln_1.weight', 'h.11.ln_2.bias', 'h.10.ln_2.weight', 'h.9.attn.c_proj.weight', 'h.8.attn.c_attn.bias', 'h.0.attn.c_attn.bias', 'h.0.ln_1.bias', 'h.14.mlp.c_fc.weight', 'h.27.ln_1.weight', 'h.1.attn.c_proj.weight', 'h.6.mlp.c_fc.weight', 'h.28.attn.c_proj.weight', 'h.11.mlp.c_proj.bias', 'h.10.mlp.c_fc.bias', 'h.20.attn.c_proj.weight', 'h.18.ln_2.bias', 'h.7.mlp.c_proj.bias', 'h.9.mlp.c_fc.bias', 'h.24.attn.c_proj.bias', 'h.27.ln_1.bias', 'h.13.attn.c_proj.bias', 'h.4.ln_1.weight', 'h.23.mlp.c_proj.weight', 'h.27.attn.c_proj.weight', 'h.11.mlp.c_fc.weight', 'h.10.attn.c_attn.weight', 'h.4.ln_1.bias', 'h.4.ln_2.bias', 'h.5.ln_2.weight', 'h.0.ln_1.weight', 'h.12.mlp.c_proj.weight', 'h.27.ln_2.weight', 'h.28.mlp.c_proj.bias', 'h.5.ln_2.bias', 'h.12.ln_2.bias', 'h.11.attn.c_proj.weight', 'h.26.ln_1.bias', 'h.16.attn.c_proj.weight', 'h.22.ln_2.bias', 'h.20.ln_2.bias', 'h.25.attn.c_proj.weight', 'h.25.ln_1.weight', 'h.9.ln_2.weight', 'h.2.mlp.c_fc.bias', 'h.28.attn.c_proj.bias', 'h.31.attn.c_attn.weight', 'h.15.mlp.c_fc.weight', 'h.10.mlp.c_proj.bias', 'h.30.mlp.c_proj.weight', 'h.31.attn.c_proj.weight', 'h.29.mlp.c_fc.bias', 'h.12.attn.c_proj.bias', 'h.19.mlp.c_proj.bias', 'h.13.attn.c_attn.weight', 'h.15.mlp.c_proj.bias', 'h.28.mlp.c_fc.bias', 'h.6.ln_1.bias', 'h.6.ln_2.weight', 'h.19.ln_1.bias', 'h.20.ln_1.bias', 'h.8.mlp.c_fc.bias', 'h.15.attn.c_attn.weight', 'h.18.ln_2.weight', 'h.1.attn.c_attn.weight', 'h.5.mlp.c_proj.weight', 'h.2.attn.c_proj.weight', 'h.13.ln_1.weight', 'h.16.mlp.c_fc.bias', 'h.13.attn.c_proj.weight', 'h.18.attn.c_proj.bias', 'h.29.mlp.c_proj.weight', 'h.18.attn.c_attn.weight', 'h.1.mlp.c_proj.bias', 'h.17.mlp.c_proj.weight', 'h.24.mlp.c_proj.bias', 'h.23.ln_1.weight', 'h.21.ln_1.bias', 'h.8.ln_1.bias', 'h.1.attn.c_attn.bias', 'h.19.mlp.c_fc.bias', 'h.20.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.24.attn.c_attn.bias', 'h.6.ln_1.weight', 'h.22.attn.c_attn.weight', 'h.26.mlp.c_proj.weight', 'h.30.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.7.ln_1.bias', 'h.18.attn.c_attn.bias', 'h.13.ln_2.bias', 'h.13.mlp.c_fc.weight', 'wpe.weight', 'h.26.ln_1.weight', 'h.23.attn.c_proj.bias', 'h.18.mlp.c_proj.weight', 'h.0.attn.c_proj.weight', 'h.17.mlp.c_proj.bias', 'h.20.attn.c_attn.weight', 'h.0.ln_2.bias', 'h.30.attn.c_attn.bias', 'h.31.mlp.c_proj.bias', 'wte.weight', 'h.15.mlp.c_proj.weight', 'h.22.mlp.c_fc.bias', 'h.21.ln_2.weight', 'h.16.mlp.c_proj.bias', 'h.15.ln_1.bias', 'h.18.mlp.c_fc.bias', 'h.22.attn.c_proj.bias', 'h.22.mlp.c_fc.weight', 'h.5.attn.c_attn.weight', 'h.8.ln_1.weight', 'h.17.mlp.c_fc.weight', 'h.4.mlp.c_fc.bias', 'h.3.mlp.c_proj.bias', 'h.24.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.29.ln_2.weight', 'h.30.mlp.c_fc.weight', 'h.3.attn.c_attn.bias', 'h.21.mlp.c_fc.weight', 'h.15.attn.c_proj.bias', 'h.21.mlp.c_proj.bias', 'h.19.attn.c_attn.bias', 'h.10.ln_1.bias', 'h.19.ln_2.bias', 'h.24.mlp.c_proj.weight', 'h.14.mlp.c_fc.bias', 'h.26.mlp.c_fc.weight', 'h.27.attn.c_attn.weight', 'h.25.mlp.c_proj.weight', 'lm_head.weight', 'h.5.mlp.c_fc.bias', 'h.20.mlp.c_fc.weight', 'h.28.ln_1.bias', 'h.31.ln_2.bias', 'h.1.mlp.c_proj.weight', 'h.0.mlp.c_proj.weight', 'h.31.ln_1.weight', 'h.8.ln_2.bias', 'h.9.ln_2.bias', 'h.31.mlp.c_fc.bias', 'h.14.attn.c_proj.bias', 'h.6.mlp.c_fc.bias', 'h.22.mlp.c_proj.bias', 'h.30.ln_2.bias', 'h.0.mlp.c_fc.weight', 'h.5.mlp.c_fc.weight', 'h.6.mlp.c_proj.bias', 'h.23.attn.c_proj.weight', 'h.16.attn.c_attn.weight', 'h.27.mlp.c_fc.bias', 'h.12.mlp.c_fc.weight', 'h.23.ln_2.bias', 'h.29.attn.c_proj.weight', 'h.18.mlp.c_proj.bias', 'ln_f.weight', 'h.15.ln_2.bias', 'h.16.ln_1.bias', 'h.8.mlp.c_fc.weight', 'h.29.attn.c_proj.bias', 'h.29.ln_2.bias', 'h.30.ln_1.weight', 'h.0.attn.c_proj.bias', 'h.19.ln_1.weight', 'h.27.attn.c_proj.bias', 'h.11.attn.c_proj.bias', 'h.4.mlp.c_fc.weight', 'h.15.ln_2.weight', 'h.23.attn.c_attn.bias', 'h.3.mlp.c_fc.weight', 'h.24.ln_1.weight', 'h.13.ln_2.weight', 'h.13.mlp.c_proj.bias', 'h.11.ln_2.weight', 'h.31.attn.c_attn.bias', 'h.27.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.5.attn.c_attn.bias', 'h.30.attn.c_attn.weight', 'h.28.mlp.c_fc.weight', 'h.26.attn.c_attn.bias', 'h.25.ln_1.bias', 'h.6.attn.c_proj.weight', 'h.7.mlp.c_proj.weight', 'h.2.mlp.c_proj.weight', 'h.19.attn.c_proj.weight', 'h.7.attn.c_proj.weight', 'h.16.ln_1.weight', 'h.11.ln_1.bias', 'h.27.mlp.c_proj.bias', 'h.26.ln_2.weight', 'h.26.attn.c_attn.weight', 'h.7.attn.c_attn.weight', 'h.31.mlp.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.7.ln_2.weight', 'h.3.ln_2.weight', 'h.12.mlp.c_proj.bias', 'h.7.attn.c_attn.bias', 'h.17.mlp.c_fc.bias', 'h.8.mlp.c_proj.weight', 'ln_f.bias', 'h.14.mlp.c_proj.weight', 'h.0.ln_2.weight', 'h.25.mlp.c_proj.bias', 'h.8.attn.c_proj.weight', 'h.14.ln_1.bias', 'h.5.ln_1.weight', 'h.7.ln_2.bias', 'h.23.mlp.c_fc.bias', 'h.2.ln_1.bias', 'h.20.mlp.c_proj.weight', 'h.26.ln_2.bias', 'h.3.mlp.c_proj.weight', 'h.20.ln_1.weight', 'h.14.attn.c_attn.bias', 'h.19.ln_2.weight', 'h.21.attn.c_attn.bias', 'h.21.attn.c_attn.weight', 'h.27.mlp.c_proj.weight', 'h.0.attn.c_attn.weight', 'h.21.attn.c_proj.weight', 'h.31.attn.c_proj.bias', 'h.4.ln_2.weight', 'h.26.attn.c_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 73.00 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 841.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNYTK/PULI-GPT-3SX\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2460\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2459\u001b[0m         )\n\u001b[0;32m-> 2460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 73.00 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.29 GiB is allocated by PyTorch, and 841.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"NYTK/PULI-GPT-3SX\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in the WikiText-2 dataset and evaluate the perplexity using a few different sliding-window strategies. Since\n",
    "this dataset is small and we're just doing one forward pass over the set, we can just load and encode the entire\n",
    "dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pysrt.open('/home/makrai/data/pair74_Gondor_freeConv_repaired_mono_noisered_beast2_TT.srt')\n",
    "sentences = pd.Series([subtitle.text for subtitle in subs])\n",
    "sentences.replace('<[^<]*<', '...', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(\"\\n\\n\".join(sentences.to_list()), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ðŸ¤— Transformers, we can simply pass the `input_ids` as the `labels` to our model, and the average negative\n",
    "log-likelihood for each token is returned as the loss. With our sliding window approach, however, there is overlap in\n",
    "the tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating\n",
    "as context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\n",
    "is an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\n",
    "for context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\n",
    "available to condition on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 256\n",
    "seq_len = encodings.input_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "print(torch.exp(torch.stack(nlls)))\n",
    "print(torch.exp(torch.stack(nlls).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window\n",
    "strategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\n",
    "and the better the reported perplexity will typically be.\n",
    "\n",
    "When we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\n",
    "as the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\n",
    "strategy, this jumps down to `16.45`. This is not only a more favorable score, but is calculated in a way that is\n",
    "closer to the true autoregressive decomposition of a sequence likelihood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
